# -*- coding: utf-8 -*-
"""create_set_gpt3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M_YSM_oW5vA6i6ZgMLOcbSZk1dZxlmlQ
"""

! pip install mlconjug3

! pip install openai

from joblib import dump, load

import re

from random import random, seed, shuffle

! pip install transformers

from transformers import GPT2TokenizerFast

from mlconjug3 import Conjugator
conjugator = Conjugator(language="it") # così usa l'italiano

def get_conj(verb):
    """
    Retourne la conjugaison du verbe.

    Args:
        verb (str): Le verbe.

    Returns:
        str: La conjugaison du verbe.

    """
    verb_conjs = conjugator.conjugate(verb).iterate()

    for verb_conj in verb_conjs:
        if verb_conj[0] == "Indicativo" and verb_conj[1] == "Indicativo presente" and verb_conj[2] == "3s":
            return verb_conj[3]
        if verb_conj[0] == "Indicativo" and verb_conj[1] == "Indicativo presente" and verb_conj[2] == "egli/ella":
            return verb_conj[3]

    return None

def build_array(sourcefile):
    """"
    The function builds an array out of an inputfile,
    one array entry per line
    """
    array_in_construction=[]
    for line in sourcefile:
        #print(line)
        cleanline=line.strip('\n ')
        array_in_construction.append(cleanline)
    return array_in_construction



def build_hypo(sourcefile):
    """"
    The function builds sets of minimal positive-negative sentences
    """
    array_in_construction=[]
    nb_line = 0
    current_set = ()
    for line in sourcefile:
        cleanline=line.strip('\n')
        #array_in_construction.append(cleanline.split('\t'))
        current_set+=(cleanline, )

        if nb_line%4 == 3:
            array_in_construction.append(current_set)
            current_set = ()

        nb_line += 1
    return array_in_construction



"""filepath = "../../gubelman_FR"

thehnamefile=open(filepath,"r")
a = build_hypo(thehnamefile)
print(a)
"""


def build_masked_sentences(hypo_sentence_available, name_available, profession_available, top_token, current_pronouns_maj, current_pronoun, mask_token):
    new_hypo_sentence_available = hypo_sentence_available.replace('NOM', name_available)
    new_hypo_sentence_available = new_hypo_sentence_available.replace('MET', profession_available)
    new_hypo_sentence_available = new_hypo_sentence_available.replace('ACTION', top_token)
    new_hypo_sentence_available = new_hypo_sentence_available.replace('PRON_maj', current_pronouns_maj)
    new_hypo_sentence_available = new_hypo_sentence_available.replace('PRON', current_pronoun)
    new_hypo_sentence_available = new_hypo_sentence_available.replace('MASK', mask_token)

    return new_hypo_sentence_available



def build_masked_context(name_available, profession_available, verb, current_pronouns_maj):
    #if verb[0] in ['a', 'e', 'i', 'o', 'u', 'h', "é", "è", "ê", "à", "â", "ô", "î", "ï", "û", "ù", "ü", "y"]:
    #    context_available = "NOM est MET qui a l'habitude d'ACTION. PRON_maj MASK vraiment souvent."
    #else:
    context_available = "NOM è MET che ha l'abitudine di ACTION. PRON_maj "# it

    new_context_sentence_available = context_available.replace('NOM', name_available)
    new_context_sentence_available = new_context_sentence_available.replace('MET', profession_available)
    new_context_sentence_available = new_context_sentence_available.replace('ACTION', verb)
    new_context_sentence_available = new_context_sentence_available.replace('PRON_maj', current_pronouns_maj)




    return new_context_sentence_available

import os
import openai

openai.api_key = "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"

tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")

frequent_verbs = ["Essere", "Avere", "Volere", "Fare", "Dire", "Capire", "Sapere", "Spiegare", "Uscire", "Andare", "Venire", "Potere", "Dovere", "Preferire", "Bere", "Salire", "Rimanere", "Morire", "Finire", "Vedere", "Leggere", "Nascere", "Chiedere", "Rispondere", "Conoscere", "Mettere", "Aprire", "Perdere", "Vivere", "Cuocere", "Scendere", "Scrivere", "Prendere", "Rompere", "Correre", "Vincere", "Tradurre", "Chiudere", "Accendere", "Crescere", "Spingere", "Scusarsi", "Chiamarsi", "Divertirsi", "Abituarsi", "Comportarsi", "Piacere", "Dispiacere", "Mancare", "Interessare", "Sembrare", "Amare", "Mangiare", "Ascoltare", "Parlare", "Ordinare", "Cercare", "Pensare", "Incontrare", "Portare", "Toccare", "Guardare", "Tornare", "Trovare", "Dare", "Abitare", "Sperare", "Aspettare", "Pranzare", "Cenare", "Telefonare", "Comprare", "Pagare", "Entrare", "Lavorare", "Studiare", "Mandare", "Lasciare", "Salutare", "Usare", "Visitare", "Camminare", "Iniziare", "Imparare", "Cambiare", "Guadagnare", "Aiutare", "Ricordare", "Tirare", "Invitare", "Diventare", "Giocare", "Credere", "Vendere", "Ripetere", "Godere", "Dormire", "Servire", "Partire", "Pulire"]

frequent_verbs = [k.lower() for k in frequent_verbs]

with open("3138_verbi_intransitivi_DISC2.txt") as testo:
    vbs = testo.read()


# pattern RE to find whole italian verbs in the list
pattern = r"\b.*[aei]?re\b"
vb_compl = re.findall(pattern, vbs)

vb_compl = [k for k in vb_compl if k in frequent_verbs]
print(vb_compl)

# for each model choose only the monotokenized verbs from the list of verbs
nb_tokens = {}

for vb in vb_compl:
  conj_vb = get_conj(vb)
  tokens = tokenizer.tokenize(f" {conj_vb}")
  nb_tokens[vb] = [len(tokens), conj_vb]

len(nb_tokens)

fName_file_path = f"100_names_f.txt"
mName_file_path = f"100_names_m.txt"
fProf_file_path = f"100_mestieri_f.txt"
mProf_file_path = f"100_mestieri_m.txt"

fName_file = open(fName_file_path, "r", encoding = "utf8")
mName_file = open(mName_file_path, "r", encoding = "utf8")
fProf_file = open(fProf_file_path, "r", encoding = "utf8")
mProf_file = open(mProf_file_path, "r", encoding = "utf8")

professionsarray = {"f": build_array(fProf_file), "m": build_array(mProf_file)} # buildarray is a function for creating lists from txt files
fnamearray = build_array(fName_file)
mnamearray = build_array(mName_file)
name_arrays = {"f": fnamearray, "m": mnamearray}
pronouns_maj = {"f": "Lei", "m": "Lui"}

list_results = []
found = False
for gender in ["f", "m"]:
            current_pronouns_maj = pronouns_maj[gender]

            count = 0

            for name_available in name_arrays[gender]:



                found = False # to stop when a good verb is found

                list_professions = professionsarray[gender]
                shuffle(list_professions)

                for profession_available in list_professions:
                    current_list_verbs = list(nb_tokens.keys()).copy()
                    shuffle(current_list_verbs)

                    if found:
                      break

                    for verb_available in current_list_verbs:
                        #print(f"current verb : {verb_available}")
                        #if not complete_check and found:
                        #    break


                        current_sentence = build_masked_context(name_available, profession_available, verb_available, current_pronouns_maj)
                        print(current_sentence)
                        count += 1


                        response = openai.Completion.create(
                                  model="text-davinci-003",
                                  prompt= current_sentence,
                                  suffix="molto spesso",
                                  temperature=0,
                                  max_tokens= nb_tokens[verb_available][0],
                                  top_p=1,
                                  frequency_penalty=0,
                                  presence_penalty=0)


                        if response["choices"][0]["text"] == nb_tokens[verb_available][1]:
                          list_results.append([name_available, profession_available, verb_available, gender, nb_tokens[verb_available][0]])
                          found = True

                          print(count)
                          count = 0
                          break


dump(list_results, "gpt_3_CpTp_list_results.joblib")

